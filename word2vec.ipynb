{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98909606-e14c-4b99-934d-d6ff94f385aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a64bb86e-3c85-4f8c-947e-cf4c1921c646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PURPOSE: load Google's pre-trained Word2Vec model.\n",
    "pretrained_model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d01cec61-4b04-4a5a-8730-facdb1d1220b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n\\nartificial intelligence ai is intelligence demonstrated by machines as opposed to the natural intelligence displayed by humans or animals', ' \\nleading ai textbooks define the field as the study of intelligent agents any system that perceives its environment and takes actions that maximize its chance of achieving its goals', ' \\nsome popular accounts use the term artificial intelligence to describe machines that mimic cognitive functions that humans associate with the human mind such as learning and problem solving however this definition is rejected by major ai researchers', '\\nai applications include advanced web search engines i', 'e', ' google recommendation systems used by youtube amazon and netflix understanding human speech such as siri or alexa selfdriving cars e', 'g', ' tesla and competing at the highest level in strategic game systems such as chess and go\\nas machines become increasingly capable tasks considered to require intelligence are often removed from the definition of ai a phenomenon known as the ai effect', '  for instance optical character recognition is frequently excluded from things considered to be ai having become a routine technology', '\\nartificial intelligence was founded as an academic discipline in 1956 and in the years since has experienced several waves of optimism followed by disappointment and the loss of funding known as an ai winter followed by new approaches success and renewed funding', ' ai research has tried and discarded many different approaches during its lifetime including simulating the brain modeling human problem solving formal logic large databases of knowledge and imitating animal behavior', ' in the first decades of the 21st century highly mathematical statistical machine learning has dominated the field and this technique has proved highly successful helping to solve many challenging problems throughout industry and academia', '\\nthe various subfields of ai research are centered around particular goals and the use of particular tools', ' the traditional goals of ai research include reasoning knowledge representation planning learning natural language processing perception and the ability to move and manipulate objects', ' general intelligence the ability to solve an arbitrary problem is among the fields longterm goals', ' to solve these problems ai researchers use versions of search and mathematical optimization formal logic artificial neural networks and methods based on statistics probability and economics', ' ai also draws upon computer science psychology linguistics philosophy and many other fields', '\\nthe field was founded on the assumption that human intelligence can be so precisely described that a machine can be made to simulate it', ' this raises philosophical arguments about the mind and the ethics of creating artificial beings endowed with humanlike intelligence', ' these issues have been explored by myth fiction and philosophy since antiquity', '\\nscience fiction and futurology have also suggested that with its enormous potential and power ai may become an existential risk to humanity', '\\nthoughtcapable artificial beings appeared as storytelling devices in antiquity and have been common in fiction as in mary shelleys frankenstein or karel čapeks r', 'u', 'r', '\\n \\nthe study of mechanical or formal reasoning began with philosophers and mathematicians in antiquity', ' the study of mathematical logic led directly to alan turings theory of computation which suggested that a machine by shuffling symbols as simple as 0 and 1 could simulate any conceivable act of mathematical deduction', ' this insight that digital computers can simulate any process of formal reasoning is known as the church–turing thesis', '\\n']\n"
     ]
    }
   ],
   "source": [
    "# PURPOSE: get data and preprocess raw text\n",
    "\n",
    "scrapped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Artificial_intelligence')\n",
    "article = scrapped_data.read()\n",
    "\n",
    "parsed_article = bs.BeautifulSoup(article,'lxml')\n",
    "\n",
    "paragraphs = parsed_article.find_all('p')\n",
    "\n",
    "\n",
    "article_text = \"\"\n",
    "\n",
    "for p in paragraphs[:10]:\n",
    "    article_text += p.text\n",
    "\n",
    "# with open('./article.txt', 'r') as file:\n",
    "#     article_text = file.read().replace('\\n', '')\n",
    "    \n",
    "article_text = re.sub('\\[.*?\\]', '', article_text)  # strip '[*]'\n",
    "article_text = re.sub('\\(|\\)|\"', '', article_text)  # strip '()' and '\"'\n",
    "raw_sentences = article_text.split('.')  # split into sentences\n",
    "\n",
    "corpus = set()\n",
    "text = []\n",
    "x = []\n",
    "\n",
    "# construct corpus and text\n",
    "for raw_sentence in raw_sentences:\n",
    "    raw_sentence = raw_sentence.translate(str.maketrans('', '', string.punctuation))  # strip all punctuation\n",
    "    raw_sentence = raw_sentence.lower()\n",
    "    x.append(raw_sentence)\n",
    "    temp = raw_sentence.split()\n",
    "    if len(temp) < 6:  # skip sentences with less than 6 words\n",
    "        continue\n",
    "    text.append(temp)\n",
    "    for i in temp:\n",
    "        corpus.add(i.lower())\n",
    "\n",
    "print(x)\n",
    "# construct word_to_index and index_to_word mapping\n",
    "word_to_index = {}\n",
    "index_to_word = {}\n",
    "index = 0\n",
    "for i in corpus:\n",
    "    word_to_index[i] = index\n",
    "    index_to_word[index] = i\n",
    "    index += 1\n",
    "    \n",
    "corpus_length = len(corpus)\n",
    "\n",
    "# softmax function\n",
    "def softmax(array):\n",
    "    ex = np.exp(array - np.max(array))\n",
    "    return ex / ex.sum()\n",
    "\n",
    "# sigmoid function for negative sampling\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "40a11b6f-d9d7-4a77-b130-33738704c91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of corpus: 289\n",
      "number of sentences: 23\n",
      "[('natural', 0), ('things', 1), ('representation', 2), ('potential', 3), ('environment', 4), ('understanding', 5), ('from', 6), ('behavior', 7), ('successful', 8), ('mind', 9)]\n",
      "[['artificial', 'intelligence', 'ai', 'is', 'intelligence', 'demonstrated', 'by', 'machines', 'as', 'opposed', 'to', 'the', 'natural', 'intelligence', 'displayed', 'by', 'humans', 'or', 'animals'], ['leading', 'ai', 'textbooks', 'define', 'the', 'field', 'as', 'the', 'study', 'of', 'intelligent', 'agents', 'any', 'system', 'that', 'perceives', 'its', 'environment', 'and', 'takes', 'actions', 'that', 'maximize', 'its', 'chance', 'of', 'achieving', 'its', 'goals'], ['some', 'popular', 'accounts', 'use', 'the', 'term', 'artificial', 'intelligence', 'to', 'describe', 'machines', 'that', 'mimic', 'cognitive', 'functions', 'that', 'humans', 'associate', 'with', 'the', 'human', 'mind', 'such', 'as', 'learning', 'and', 'problem', 'solving', 'however', 'this', 'definition', 'is', 'rejected', 'by', 'major', 'ai', 'researchers'], ['ai', 'applications', 'include', 'advanced', 'web', 'search', 'engines', 'i'], ['google', 'recommendation', 'systems', 'used', 'by', 'youtube', 'amazon', 'and', 'netflix', 'understanding', 'human', 'speech', 'such', 'as', 'siri', 'or', 'alexa', 'selfdriving', 'cars', 'e'], ['tesla', 'and', 'competing', 'at', 'the', 'highest', 'level', 'in', 'strategic', 'game', 'systems', 'such', 'as', 'chess', 'and', 'go', 'as', 'machines', 'become', 'increasingly', 'capable', 'tasks', 'considered', 'to', 'require', 'intelligence', 'are', 'often', 'removed', 'from', 'the', 'definition', 'of', 'ai', 'a', 'phenomenon', 'known', 'as', 'the', 'ai', 'effect'], ['for', 'instance', 'optical', 'character', 'recognition', 'is', 'frequently', 'excluded', 'from', 'things', 'considered', 'to', 'be', 'ai', 'having', 'become', 'a', 'routine', 'technology'], ['artificial', 'intelligence', 'was', 'founded', 'as', 'an', 'academic', 'discipline', 'in', '1956', 'and', 'in', 'the', 'years', 'since', 'has', 'experienced', 'several', 'waves', 'of', 'optimism', 'followed', 'by', 'disappointment', 'and', 'the', 'loss', 'of', 'funding', 'known', 'as', 'an', 'ai', 'winter', 'followed', 'by', 'new', 'approaches', 'success', 'and', 'renewed', 'funding'], ['ai', 'research', 'has', 'tried', 'and', 'discarded', 'many', 'different', 'approaches', 'during', 'its', 'lifetime', 'including', 'simulating', 'the', 'brain', 'modeling', 'human', 'problem', 'solving', 'formal', 'logic', 'large', 'databases', 'of', 'knowledge', 'and', 'imitating', 'animal', 'behavior'], ['in', 'the', 'first', 'decades', 'of', 'the', '21st', 'century', 'highly', 'mathematical', 'statistical', 'machine', 'learning', 'has', 'dominated', 'the', 'field', 'and', 'this', 'technique', 'has', 'proved', 'highly', 'successful', 'helping', 'to', 'solve', 'many', 'challenging', 'problems', 'throughout', 'industry', 'and', 'academia'], ['the', 'various', 'subfields', 'of', 'ai', 'research', 'are', 'centered', 'around', 'particular', 'goals', 'and', 'the', 'use', 'of', 'particular', 'tools'], ['the', 'traditional', 'goals', 'of', 'ai', 'research', 'include', 'reasoning', 'knowledge', 'representation', 'planning', 'learning', 'natural', 'language', 'processing', 'perception', 'and', 'the', 'ability', 'to', 'move', 'and', 'manipulate', 'objects'], ['general', 'intelligence', 'the', 'ability', 'to', 'solve', 'an', 'arbitrary', 'problem', 'is', 'among', 'the', 'fields', 'longterm', 'goals'], ['to', 'solve', 'these', 'problems', 'ai', 'researchers', 'use', 'versions', 'of', 'search', 'and', 'mathematical', 'optimization', 'formal', 'logic', 'artificial', 'neural', 'networks', 'and', 'methods', 'based', 'on', 'statistics', 'probability', 'and', 'economics'], ['ai', 'also', 'draws', 'upon', 'computer', 'science', 'psychology', 'linguistics', 'philosophy', 'and', 'many', 'other', 'fields'], ['the', 'field', 'was', 'founded', 'on', 'the', 'assumption', 'that', 'human', 'intelligence', 'can', 'be', 'so', 'precisely', 'described', 'that', 'a', 'machine', 'can', 'be', 'made', 'to', 'simulate', 'it'], ['this', 'raises', 'philosophical', 'arguments', 'about', 'the', 'mind', 'and', 'the', 'ethics', 'of', 'creating', 'artificial', 'beings', 'endowed', 'with', 'humanlike', 'intelligence'], ['these', 'issues', 'have', 'been', 'explored', 'by', 'myth', 'fiction', 'and', 'philosophy', 'since', 'antiquity'], ['science', 'fiction', 'and', 'futurology', 'have', 'also', 'suggested', 'that', 'with', 'its', 'enormous', 'potential', 'and', 'power', 'ai', 'may', 'become', 'an', 'existential', 'risk', 'to', 'humanity'], ['thoughtcapable', 'artificial', 'beings', 'appeared', 'as', 'storytelling', 'devices', 'in', 'antiquity', 'and', 'have', 'been', 'common', 'in', 'fiction', 'as', 'in', 'mary', 'shelleys', 'frankenstein', 'or', 'karel', 'čapeks', 'r'], ['the', 'study', 'of', 'mechanical', 'or', 'formal', 'reasoning', 'began', 'with', 'philosophers', 'and', 'mathematicians', 'in', 'antiquity'], ['the', 'study', 'of', 'mathematical', 'logic', 'led', 'directly', 'to', 'alan', 'turings', 'theory', 'of', 'computation', 'which', 'suggested', 'that', 'a', 'machine', 'by', 'shuffling', 'symbols', 'as', 'simple', 'as', '0', 'and', '1', 'could', 'simulate', 'any', 'conceivable', 'act', 'of', 'mathematical', 'deduction'], ['this', 'insight', 'that', 'digital', 'computers', 'can', 'simulate', 'any', 'process', 'of', 'formal', 'reasoning', 'is', 'known', 'as', 'the', 'church–turing', 'thesis']]\n"
     ]
    }
   ],
   "source": [
    "print(\"length of corpus: {}\".format(corpus_length))\n",
    "print(\"number of sentences: {}\".format(len(text)))\n",
    "print(list(word_to_index.items())[:10])\n",
    "#print(corpus)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bdb74b88-5784-4346-b124-62f7bc872b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PURPOSE: construct CBOW model class\n",
    "\n",
    "class CBOW_model:\n",
    "    def __init__(self):\n",
    "        self.x_train = []\n",
    "        self.y_train = []\n",
    "        self.y_predict = []\n",
    "        self.hidden = []\n",
    "        self.output = []\n",
    "        self.W1 = []\n",
    "        self.W2 = []\n",
    "        self.embedding_dimension = 0\n",
    "        self.window_size = 0\n",
    "        self.alpha = 0\n",
    "        self.epoch = 0\n",
    "        self.negative_sampling = False\n",
    "    \n",
    "    # inialize the model eg. train & window_size & alpha, etc.  \n",
    "    def initialize(self, window_size=1, negative_sampling=False, embedding_dimension=300, alpha=1, epoch=1000, google_news=False):\n",
    "        # initialize hyperparameters\n",
    "        self.window_size = window_size\n",
    "        self.negative_sampling = negative_sampling\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.alpha = alpha\n",
    "        self.epoch = epoch\n",
    "        \n",
    "        # generate one-hot vectors for target word (y) and context word (x)\n",
    "        for sentence in text:\n",
    "            for index in range(len(sentence)):\n",
    "                target_word = sentence[index]\n",
    "                # target one-hot vector\n",
    "                target_vector = np.zeros(corpus_length, dtype=int)\n",
    "                target_vector[word_to_index[target_word]] = 1\n",
    "                self.y_train.append(target_vector)\n",
    "                # context one-hot vector\n",
    "                context_word = set(sentence[index-window_size:index] + sentence[index+1:index+window_size+1])\n",
    "                context_vector = np.zeros(corpus_length, dtype=int)\n",
    "                for i in context_word:\n",
    "                    context_vector[word_to_index[i]] = 1\n",
    "                # early averaging the context vector due to nature of multiple word CBOW model\n",
    "                context_vector = context_vector / len(context_word)\n",
    "                self.x_train.append(context_vector)\n",
    "        \n",
    "        # initialize weight metrics\n",
    "        self.W1 = np.random.uniform(-0.8, 0.8, (corpus_length, self.embedding_dimension))\n",
    "        self.W2 = np.random.uniform(-0.8, 0.8, (self.embedding_dimension, corpus_length))\n",
    "        \n",
    "        # use Google-news pretrained data\n",
    "        if google_news:\n",
    "            for word in corpus:\n",
    "                index = word_to_index[word]\n",
    "                if word in pretrained_model:\n",
    "                    self.W1[index] = pretrained_model[word]\n",
    "        \n",
    "    \n",
    "    # forward propagation\n",
    "    def forward_propagation(self, x):\n",
    "        X = self.x_train[x]\n",
    "        self.hidden = np.dot(self.W1.T, X)\n",
    "        self.output = np.dot(self.W2.T, self.hidden)\n",
    "        self.y_predict = softmax(self.output)\n",
    "    \n",
    "    # backward propagation\n",
    "    def backward_propagation(self, x):\n",
    "        e = self.y_predict - self.y_train[x]\n",
    "        dW2 = np.outer(self.hidden, e)\n",
    "        dW1 = np.outer(self.x_train[x], np.dot(self.W2, e))\n",
    "        self.W2 = self.W2 - self.alpha * dW2\n",
    "        self.W1 = self.W1 - self.alpha * dW1\n",
    "        \n",
    "    # calculate loss\n",
    "    def calculate_loss(self, x):\n",
    "#         sum1 = 0\n",
    "#         target_word_one_hot_vector = self.y_train[x]\n",
    "#         for i in range(len(target_word_one_hot_vector)):\n",
    "#             if target_word_one_hot_vector[i] == 1:\n",
    "#                 sum1 = -self.output[i]\n",
    "#                 break\n",
    "#         sum2 = np.log(np.sum(np.exp(self.output)))\n",
    "#         return sum1 + sum2\n",
    "        target_word = ''\n",
    "        output = 0\n",
    "        target_word_one_hot_vector = self.y_train[x]\n",
    "        for i in range(len(target_word_one_hot_vector)):\n",
    "            if target_word_one_hot_vector[i] == 1:\n",
    "                # target_word = index_to_word[i]\n",
    "                output = -np.log(self.y_predict[i])\n",
    "                # print('target word {}, loss = {}'.format(target_word, output))\n",
    "                return output, i\n",
    "        \n",
    "    \n",
    "    # training the model\n",
    "    def train(self):\n",
    "        for i in range(self.epoch):\n",
    "            epoch_loss = 0\n",
    "            correct = 0\n",
    "            for x in range(len(self.x_train)):\n",
    "                self.forward_propagation(x)\n",
    "                self.backward_propagation(x)\n",
    "                current_loss, index = self.calculate_loss(x)\n",
    "                epoch_loss += current_loss\n",
    "                if np.argmax(self.y_predict) == index:\n",
    "                    correct += 1\n",
    "            epoch_loss /= len(self.x_train)\n",
    "            if i % 100 == 0 or i == self.epoch - 1:\n",
    "                print('epoch{} loss: {} accuracy: {}'.format(i, epoch_loss, correct / len(self.x_train)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7408408f-353b-48a5-a730-90ba06d8bddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0 loss: 6.985438459959627 accuracy: 0.0018484288354898336\n",
      "epoch100 loss: 0.169901391136197 accuracy: 0.9926062846580407\n",
      "epoch200 loss: 0.07027234811183239 accuracy: 0.9926062846580407\n",
      "epoch300 loss: 0.04560652987982047 accuracy: 0.9926062846580407\n",
      "epoch400 loss: 0.034569372398902926 accuracy: 0.9926062846580407\n",
      "epoch499 loss: 0.02838439404600512 accuracy: 0.9926062846580407\n"
     ]
    }
   ],
   "source": [
    "cbow_model = CBOW_model()\n",
    "cbow_model.initialize(alpha=0.01, epoch=500, google_news=False, window_size=3)\n",
    "cbow_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9261fc79-3f9d-4c89-a7fc-754cac4e7c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.5 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0. ]\n"
     ]
    }
   ],
   "source": [
    "print(cbow_model.x_train[57])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f05bc4a3-6a15-495d-afeb-fba2002bff53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999688851741054\n",
      "0.9999999999999998\n",
      "its\n"
     ]
    }
   ],
   "source": [
    "print(predict(cbow_model, ['maximize', 'chance']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6c80cae3-2a8c-4600-b118-52ad6014d111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PURPOSE: construct CBOW model class with negative sampling\n",
    "\n",
    "class CBOW_model_negative_sampling:\n",
    "    def __init__(self):\n",
    "        self.x_train_one_hot = []\n",
    "        self.x_train_context_words = []\n",
    "        self.y_train_positive = []\n",
    "        self.y_train_negative = []\n",
    "        self.y_predict = []\n",
    "        self.hidden = []\n",
    "        self.output = []\n",
    "        self.cost = 0\n",
    "        self.W1 = []\n",
    "        self.W2 = []\n",
    "        self.embedding_dimension = 0\n",
    "        self.window_size = 0\n",
    "        self.alpha = 0\n",
    "        self.epoch = 0\n",
    "        self.Pnw = {}\n",
    "        self.k = 0\n",
    "    \n",
    "    # inialize the model eg. train & window_size & alpha, etc.  \n",
    "    def initialize(self, window_size=1, embedding_dimension=300, alpha=0.01, epoch=1000, google_news=False, k=10):\n",
    "        # initialize hyperparameters\n",
    "        self.window_size = window_size\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.alpha = alpha\n",
    "        self.epoch = epoch\n",
    "        self.k = k\n",
    "        \n",
    "        # compute Pnw\n",
    "        power = 3 / 4\n",
    "        frequency = {}\n",
    "        raw_text = []\n",
    "        for sentence in text:\n",
    "            for word in sentence:\n",
    "                raw_text.append(word)\n",
    "\n",
    "        for word in raw_text:\n",
    "            if word in frequency:\n",
    "                frequency[word] += 1\n",
    "            else:\n",
    "                frequency[word] = 1\n",
    "        for key in frequency:\n",
    "            frequency[key] /= len(raw_text)\n",
    "        temp = {key: val ** power for key, val in frequency.items()}\n",
    "        Z = sum(temp.values())\n",
    "        self.Pnw = {key: val / Z for key, val in temp.items()}\n",
    "        print(sum(self.Pnw.values()))\n",
    "        \n",
    "        # generate one-hot vectors for target word (y_train_positive & y_train_negative) and context word (x_train)\n",
    "        for sentence in text:\n",
    "            for index in range(len(sentence)):\n",
    "                target_word = sentence[index]\n",
    "                # context one-hot vector x\n",
    "                context_word = set(sentence[index-window_size:index] + sentence[index+1:index+window_size+1])\n",
    "                context_vector = np.zeros(corpus_length, dtype=int)\n",
    "                for i in context_word:\n",
    "                    context_vector[word_to_index[i]] = 1\n",
    "                # early averaging the context vector due to nature of multiple word CBOW model\n",
    "                context_vector = context_vector / len(context_word)\n",
    "                self.x_train_one_hot.append(context_vector)\n",
    "                self.x_train_context_words.append(context_word)\n",
    "                # y_train_positive\n",
    "                self.y_train_positive.append(target_word)\n",
    "                # y_train_negative\n",
    "                number_of_negative_words = self.k\n",
    "                negative_words = []\n",
    "                while number_of_negative_words > 0: \n",
    "                    choice = np.random.choice(list(self.Pnw.keys()), p=list(self.Pnw.values()))\n",
    "                    if choice == target_word or choice in context_word:\n",
    "                        continue\n",
    "                    else:\n",
    "                        negative_words.append(choice)\n",
    "                        number_of_negative_words -= 1\n",
    "                self.y_train_negative.append(negative_words)\n",
    "                \n",
    "        \n",
    "        # initialize weight metrics\n",
    "        self.W1 = np.random.uniform(-0.8, 0.8, (self.embedding_dimension, corpus_length))\n",
    "        self.W2 = np.random.uniform(-0.8, 0.8, (corpus_length, self.embedding_dimension))\n",
    "        \n",
    "        # use Google-news pretrained data\n",
    "        if google_news:\n",
    "            for word in corpus:\n",
    "                index = word_to_index[word]\n",
    "                if word in pretrained_model:\n",
    "                    self.W1[:,index] = pretrained_model[word]\n",
    "                       \n",
    "                    \n",
    "    # forward propagation\n",
    "    def forward_propagation(self, x):\n",
    "        X = self.x_train_one_hot[x]\n",
    "        self.hidden = np.dot(self.W1, X)\n",
    "        c_positive = self.W2[word_to_index[self.y_train_positive[x]]]\n",
    "        pos_cost = -np.log(sigmoid(np.dot(c_positive, self.hidden)))\n",
    "        neg_cost = 0\n",
    "        for negative_word in self.y_train_negative[x]:\n",
    "            c_negative = self.W2[word_to_index[negative_word]]\n",
    "            neg_cost += np.log(sigmoid(np.dot(-c_negative, self.hidden)))\n",
    "        self.cost = pos_cost - neg_cost\n",
    "\n",
    "    # backward propagation\n",
    "    def backward_propagation(self, x):\n",
    "        c_positive_index = word_to_index[self.y_train_positive[x]]\n",
    "        # compute W1\n",
    "        dW1 = (sigmoid(np.dot(self.W2[c_positive_index], self.hidden)) - 1) * self.W2[c_positive_index]\n",
    "        # compute W2\n",
    "        self.W2[c_positive_index] = self.W2[c_positive_index] - self.alpha * ((sigmoid(np.dot(self.W2[c_positive_index], self.hidden))) - 1) * self.hidden\n",
    "        for negative_word in self.y_train_negative[x]:\n",
    "            c_negative_index = word_to_index[negative_word]\n",
    "            # compute W1\n",
    "            dW1 += sigmoid(np.dot(self.W2[c_negative_index], self.hidden)) * self.W2[c_negative_index]\n",
    "            # compute W2\n",
    "            self.W2[c_negative_index] = self.W2[c_negative_index] - self.alpha * sigmoid(np.dot(self.W2[c_negative_index], self.hidden)) * self.hidden\n",
    "\n",
    "        # update W1\n",
    "        for word in self.x_train_context_words[x]:\n",
    "            context_word_index = word_to_index[word]\n",
    "            self.W1[:,context_word_index] = self.W1[:,context_word_index] - 1 / len(self.x_train_context_words[x]) * self.alpha * dW1\n",
    "\n",
    "\n",
    "    # make prediction\n",
    "    def predict(self, x):\n",
    "        X = self.x_train_one_hot[x]\n",
    "        hidden = np.dot(self.W1, X)\n",
    "        output = np.dot(self.W2, hidden)\n",
    "        y_predict = softmax(output)\n",
    "        ans = index_to_word[np.argmax(y_predict)]\n",
    "        return ans == self.y_train_positive[x]\n",
    "\n",
    "\n",
    "\n",
    "    # training the model\n",
    "    def train(self):\n",
    "        for i in range(self.epoch):\n",
    "            for x in range(len(self.x_train_one_hot)):\n",
    "                self.forward_propagation(x)\n",
    "                self.backward_propagation(x)\n",
    "            if i % 100 == 0 or i == self.epoch - 1:\n",
    "                correct = 0\n",
    "                for index in range(len(self.x_train_one_hot)):\n",
    "                    if self.predict(index):\n",
    "                        correct += 1\n",
    "                print('epoch{} accuracy: {}'.format(i, correct / len(self.x_train_one_hot)))\n",
    "                \n",
    "    def debug(self, x):\n",
    "        X = self.x_train_one_hot[x]\n",
    "        hidden = np.dot(self.W1, X)\n",
    "        output = np.dot(self.W2, hidden)\n",
    "        y_predict = softmax(output)\n",
    "        ans = index_to_word[np.argmax(y_predict)]\n",
    "        print('{} --- {}'.format(ans, self.y_train_positive[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cfd41236-a9ca-48c3-8eb3-4042a3826423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999999933\n",
      "epoch0 accuracy: 0.0018484288354898336\n",
      "epoch100 accuracy: 0.059149722735674676\n",
      "epoch200 accuracy: 0.1756007393715342\n",
      "epoch300 accuracy: 0.27911275415896486\n",
      "epoch400 accuracy: 0.34935304990757854\n",
      "epoch500 accuracy: 0.38817005545286504\n",
      "epoch600 accuracy: 0.41589648798521256\n",
      "epoch700 accuracy: 0.4658040665434381\n",
      "epoch800 accuracy: 0.4953789279112754\n",
      "epoch900 accuracy: 0.5138632162661737\n",
      "epoch1000 accuracy: 0.5268022181146026\n",
      "epoch1100 accuracy: 0.532347504621072\n",
      "epoch1200 accuracy: 0.5378927911275416\n",
      "epoch1300 accuracy: 0.5471349353049908\n",
      "epoch1400 accuracy: 0.55637707948244\n",
      "epoch1500 accuracy: 0.5619223659889094\n",
      "epoch1600 accuracy: 0.5693160813308688\n",
      "epoch1700 accuracy: 0.5767097966728281\n",
      "epoch1800 accuracy: 0.5804066543438078\n",
      "epoch1900 accuracy: 0.5841035120147874\n",
      "epoch2000 accuracy: 0.5859519408502772\n",
      "epoch2100 accuracy: 0.589648798521257\n",
      "epoch2200 accuracy: 0.5878003696857671\n",
      "epoch2300 accuracy: 0.589648798521257\n",
      "epoch2400 accuracy: 0.5914972273567468\n",
      "epoch2500 accuracy: 0.5951940850277264\n",
      "epoch2600 accuracy: 0.5970425138632163\n",
      "epoch2700 accuracy: 0.5988909426987061\n",
      "epoch2800 accuracy: 0.600739371534196\n",
      "epoch2900 accuracy: 0.6025878003696857\n",
      "epoch3000 accuracy: 0.6044362292051756\n",
      "epoch3100 accuracy: 0.6062846580406654\n",
      "epoch3200 accuracy: 0.6062846580406654\n",
      "epoch3300 accuracy: 0.6081330868761553\n",
      "epoch3400 accuracy: 0.6081330868761553\n",
      "epoch3500 accuracy: 0.6062846580406654\n",
      "epoch3600 accuracy: 0.6081330868761553\n",
      "epoch3700 accuracy: 0.609981515711645\n",
      "epoch3800 accuracy: 0.6118299445471349\n",
      "epoch3900 accuracy: 0.6136783733826248\n",
      "epoch4000 accuracy: 0.6136783733826248\n",
      "epoch4100 accuracy: 0.6136783733826248\n",
      "epoch4200 accuracy: 0.6192236598890942\n",
      "epoch4300 accuracy: 0.6210720887245841\n",
      "epoch4400 accuracy: 0.6229205175600739\n",
      "epoch4500 accuracy: 0.6266173752310537\n",
      "epoch4600 accuracy: 0.6284658040665434\n",
      "epoch4700 accuracy: 0.6284658040665434\n",
      "epoch4800 accuracy: 0.6284658040665434\n",
      "epoch4900 accuracy: 0.6284658040665434\n",
      "epoch4999 accuracy: 0.6303142329020333\n"
     ]
    }
   ],
   "source": [
    "cbow_model_negative_sampling = CBOW_model_negative_sampling()\n",
    "cbow_model_negative_sampling.initialize(alpha=0.01, epoch=5000, google_news=False, window_size=2, embedding_dimension=300)\n",
    "#print(cbow_model_negative_sampling.y_train_negative)\n",
    "cbow_model_negative_sampling.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "117d6f3e-c3f4-4970-b26a-d68211389c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intelligence', 'use', 'the', 'artificial'}\n",
      "term\n",
      "['centered', 'such', 'go', 'competing', 'and', 'reasoning', 'web', 'tasks', 'imitating', 'machines']\n"
     ]
    }
   ],
   "source": [
    "print(cbow_model_negative_sampling.x_train_context_words[53])\n",
    "print(cbow_model_negative_sampling.y_train_positive[53])\n",
    "print(cbow_model_negative_sampling.y_train_negative[53])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8927f7cc-e08a-47ff-ae30-eccca13ba0f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
